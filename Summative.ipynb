{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9363c26",
   "metadata": {},
   "source": [
    "# AIM: Recommend Relevant Businesses To Users Based On The Activity Of Users On Yelp \n",
    "\n",
    "### Activity = reviews given by users to businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c835eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud\n",
    "# !pip install mpl_toolkits\n",
    "# !pip install surprise\n",
    "# !pip install gensim\n",
    "# !pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import collections\n",
    "import re, string\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import Dataset\n",
    "from surprise import BaselineOnly\n",
    "#from surprise.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import torch.nn as nn\n",
    "from sklearn import model_selection,metrics,preprocessing\n",
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6ed35a",
   "metadata": {},
   "source": [
    "## PRE-PROCESSING TECHNIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13adf908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================================\n",
    "# Importing Yelp dataset into Dataframe\n",
    "#=================================================================\n",
    "\n",
    "def init_ds(json):\n",
    "    ds= {}\n",
    "    keys = json.keys()\n",
    "    for k in keys:\n",
    "        ds[k]= []\n",
    "    return ds, keys\n",
    "\n",
    "#=================================================================\n",
    "# Converting Json files to pd.Dataframes\n",
    "#=================================================================\n",
    "def read_json(file):\n",
    "    dataset = {}\n",
    "    keys = []\n",
    "    with open(file, encoding='utf-8') as file_lines:\n",
    "        for count, line in enumerate(file_lines):\n",
    "            data = json.loads(line.strip())\n",
    "            if count ==0:\n",
    "                dataset, keys = init_ds(data)\n",
    "            for k in keys:\n",
    "                dataset[k].append(data[k])\n",
    "                \n",
    "        return pd.DataFrame(dataset)\n",
    "\n",
    "# Import yelp business\n",
    "yelp_business = read_json('yelp_dataset\\yelp_dataset\\yelp_academic_dataset_business.json')\n",
    "\n",
    "#Import yelp_review\n",
    "yelp_review= read_json('yelp_dataset\\yelp_dataset\\yelp_academic_dataset_review.json')\n",
    "\n",
    "#Import yelp_user\n",
    "yelp_user = read_json('yelp_dataset\\yelp_dataset\\yelp_academic_dataset_user.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4903dabb",
   "metadata": {},
   "source": [
    "### Average Star Rating Given By Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================\n",
    "# getting the average star given by the users from\n",
    "# yelp_review dataset (dataset with the review in)\n",
    "#==============================================\n",
    "list_star_count = yelp_review['stars'].tolist()\n",
    "plt.hist(list_star_count, bins = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a7e29",
   "metadata": {},
   "source": [
    "### Yelping Since"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================\n",
    "# getting the number of reviews each year\n",
    "# given by the user\n",
    "#==============================================\n",
    "list_date_count = pd.DatetimeIndex(yelp_review['date']).year.tolist()\n",
    "counts = pd.Series(list_date_count).value_counts()\n",
    "counts.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640d5bc",
   "metadata": {},
   "source": [
    "### Average Reviews Received By Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59625c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================\n",
    "# getting the number of reviews (x coordinate)\n",
    "# received by number of businesses businesses (y coordinate)\n",
    "#==============================================\n",
    "list_business_review_count = yelp_business['review_count'].tolist()\n",
    "plt.hist(list_business_review_count, bins = 5,range= (0,20))\n",
    "plt.hist(list_business_review_count, bins = 5,range= (20,50))\n",
    "plt.hist(list_business_review_count, bins = 5,range= (50,100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a4287",
   "metadata": {},
   "source": [
    "### Filtering Restaurants Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce06e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_business = yelp_business.loc[(yelp_business['categories'].str.contains('Food')) | (yelp_business['categories'].str.contains('Restaurants'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4fa77",
   "metadata": {},
   "source": [
    "#### Review Count For The Filtered Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_business_review_counts = yelp_business['review_count'].tolist()\n",
    "plt.hist(list_business_review_counts, bins = 5,range= (0,20))\n",
    "plt.hist(list_business_review_counts, bins = 5,range= (20,50))\n",
    "plt.hist(list_business_review_counts, bins = 5,range= (50,100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2498657",
   "metadata": {},
   "source": [
    "#### Filter Businesses with 20 or more reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba49e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_business = yelp_business.loc[(yelp_business['review_count']>= 20)]\n",
    "yelp_business.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b713c",
   "metadata": {},
   "source": [
    "### Filtering User Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a019e72",
   "metadata": {},
   "source": [
    "#### Average Reviews Given By Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_review_count = yelp_user['review_count'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yelp_user['review_count'].min(),yelp_user['review_count'].max(),yelp_user['review_count'].mean())\n",
    "\n",
    "plt.hist(list_review_count, bins = 5,range= (0,20))\n",
    "plt.hist(list_review_count, bins = 5,range= (20,50))\n",
    "plt.hist(list_review_count, bins = 5,range= (50,100))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa1274",
   "metadata": {},
   "source": [
    "#### Filtering Users with 20 or more reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4befc78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_user = yelp_user[yelp_user.review_count >= 20]\n",
    "list_review_count = yelp_user['review_count'].tolist()\n",
    "plt.hist(list_review_count, bins = 5,range= (0,19))\n",
    "plt.hist(list_review_count, bins = 5,range= (20,50))\n",
    "plt.hist(list_review_count, bins = 5,range= (50,100))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f150c",
   "metadata": {},
   "source": [
    "### Filtering yelp_review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a960d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering yelp-review dataset with user_ids that are present in business_id and yelp_review datasets\n",
    "# basicaly getting users users with more than 20 reviews and restaurants with more than 20 reviews into a dataset\n",
    "yelp_review = yelp_review.loc[(yelp_review['user_id'].isin(yelp_user['user_id'])) & (yelp_review['business_id'].isin(yelp_business['business_id']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9d56e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb98c2d",
   "metadata": {},
   "source": [
    "## Preparing New Dataset to User for Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81609a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================\n",
    "# Getting a smaller sample of dataset so it's easier to handle\n",
    "#================================================================\n",
    "\n",
    "sample = yelp_review.head(int(yelp_review.shape[0]/100))\n",
    "\n",
    "#================================================================\n",
    "# Renaming all the user Ids so it's easier to handle\n",
    "#================================================================\n",
    "\n",
    "user_id_unique = sample.user_id.unique()\n",
    "\n",
    "user_dictionary = {}\n",
    "\n",
    "for i in range(len(user_id_unique)):\n",
    "    user_dictionary[user_id_unique[i]] = i\n",
    "        \n",
    "sample = sample.replace({\"user_id\": user_dictionary})\n",
    "\n",
    "yelp_user = yelp_user.replace({\"user_id\": user_dictionary})\n",
    "\n",
    "#================================================================\n",
    "# Renaming all the business Ids so it's easier to handle\n",
    "#================================================================\n",
    "\n",
    "business_id_unique = sample.business_id.unique()\n",
    "\n",
    "business_dictionary = {}\n",
    "\n",
    "for i in range(len(business_id_unique)):\n",
    "    business_dictionary[business_id_unique[i]] = i\n",
    "        \n",
    "sample = sample.replace({\"business_id\": business_dictionary})\n",
    "\n",
    "yelp_business = yelp_business.replace({\"business_id\": business_dictionary})\n",
    "\n",
    "#================================================================\n",
    "# Obtaining user and business dataset with users in the sample \n",
    "# dataset above\n",
    "#================================================================\n",
    "\n",
    "yelp_user_filtered = yelp_user.loc[yelp_user['user_id'].isin(sample['user_id'])]\n",
    "\n",
    "yelp_business_filtered = yelp_business.loc[yelp_business['business_id'].isin(sample['business_id'])]\n",
    "\n",
    "#================================================================\n",
    "# Saving these datasets into a csv file to be used for the summative\n",
    "#================================================================\n",
    "\n",
    "sample.to_csv('sample.csv')\n",
    "\n",
    "yelp_user_filtered.to_csv('yelp_user_filtered.csv')\n",
    "\n",
    "yelp_business_filtered.to_csv('yelp_business_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39786b81",
   "metadata": {},
   "source": [
    "# Building Recommender System -- Run From Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bfce55",
   "metadata": {},
   "source": [
    "1. Cold Start Problem Solved - KMeans Clustering Algorithm\n",
    "2. Non personalised Recommender System\n",
    "3. Personalised Recommender System - SVD\n",
    "4. Personalised Recommender System - Neural Network, Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f0e6c",
   "metadata": {},
   "source": [
    "## 1. Cold start solved (KMeans Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# Reading CSV file, my own mini version of Yelp after\n",
    "# preprocessing\n",
    "#============================================================\n",
    "\n",
    "sample = pd.read_csv('sample.csv')\n",
    "\n",
    "yelp_user_filtered = pd.read_csv('yelp_user_filtered.csv')\n",
    "\n",
    "yelp_business_filtered = pd.read_csv('yelp_business_filtered.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0424ccc",
   "metadata": {},
   "source": [
    "#### a) Visualising the number of restaurnats in all the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================================================\n",
    "# plot of the latitude and longitude of all the restaurants\n",
    "#===================================================================================\n",
    "plt.scatter(yelp_business_filtered['latitude'],yelp_business_filtered['longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ff337",
   "metadata": {},
   "source": [
    "#### b) Visualising the number restaurants in a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a62dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================================================\n",
    "# plot of the latitude and longitude of the restaurants in a specific state\n",
    "#===================================================================================\n",
    "state_business = yelp_business_filtered.loc[yelp_business_filtered['state'] == 'PA']\n",
    "plt.scatter(state_business['latitude'],state_business['longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93425a99",
   "metadata": {},
   "source": [
    "#### c) Visualising the number restaurants in a city of a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================================================\n",
    "# plot of the latitude and longitude of the restaurants in a specific city\n",
    "#===================================================================================\n",
    "city_business = yelp_business_filtered.loc[yelp_business_filtered['city'] == 'Edmonton']\n",
    "plt.scatter(city_business['latitude'],city_business['longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55464071",
   "metadata": {},
   "source": [
    "#### d) KMeans Clustering Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================\n",
    "#============= K-Nearest Neighbour Algorithm ====================\n",
    "#      this is to find the clusters of location\n",
    "#================================================================\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def distortion_counter(min,max,dataset):\n",
    "    distortions = []\n",
    "    K = range(min,max)\n",
    "    for k in K:\n",
    "        kmeansModel = KMeans(n_clusters = k)\n",
    "        KmeansModel = kmeansModel.fit(dataset)\n",
    "        distortions.append(kmeansModel.inertia_)\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(12,8))\n",
    "    plt.plot(K,distortions,marker='o')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortions')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "#==================================================================\n",
    "# grouping for cities using KMeans\n",
    "#==================================================================\n",
    "\n",
    "# getting the number of city\n",
    "m_clus = int((1/4)*len(yelp_business_filtered['city'].unique()))\n",
    "kmeans_city = KMeans(n_clusters=m_clus,init='k-means++')\n",
    "kmeans_city.fit(yelp_business_filtered[['latitude','longitude']])\n",
    "x = kmeans_city.labels_\n",
    "yelp_business_filtered['city_category'] = x\n",
    "\n",
    "#==================================================================\n",
    "# This is to solve the cold start problem\n",
    "# Recommendation based on location\n",
    "#==================================================================\n",
    "\n",
    "def increase_radius(lat,long,number):\n",
    "    business_id = []\n",
    "    n = 0.01\n",
    "    while len(business_id) <= number:\n",
    "        \n",
    "        #==================================================\n",
    "        # lat values\n",
    "        lat_values = [lat + n, lat - n]\n",
    "        #==================================================\n",
    "        # long values\n",
    "        long_values = [long + n, long - n]\n",
    "        #==================================================\n",
    "        # for loop to check the regions\n",
    "        for lat in lat_values:\n",
    "            for long in long_values:\n",
    "                #==================================================\n",
    "                # get the region of cluster\n",
    "                cluster = kmeans_city.predict(np.array([long,lat]).reshape(1,-1))[0]\n",
    "                #==================================================\n",
    "                # getting the number of number of restaurants for that city cluster\n",
    "                restaurants = yelp_business_filtered[yelp_business_filtered['city_category']==cluster]\n",
    "                #==================================================\n",
    "                # number of restaurants in that region\n",
    "                n_o_restaurants = restaurants.shape[0]\n",
    "\n",
    "                list_of_new_id = restaurants['business_id'].values.tolist()\n",
    "                res = [x for x in list_of_new_id + business_id if x not in business_id]\n",
    "                business_id_with_more_star = []\n",
    "                for i in res:\n",
    "                    star_now = yelp_business_filtered['stars'][yelp_business_filtered['business_id']==i]\n",
    "                    if int(star_now) >= 4:\n",
    "                        business_id_with_more_star.append(i)\n",
    "                if len(res) == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    for i in range(len(business_id_with_more_star)):\n",
    "                        business_id.append(business_id_with_more_star[i])\n",
    "        n = n + 0.01\n",
    "        \n",
    "    return business_id\n",
    "\n",
    "def new_user():\n",
    "    #=================================================================================================\n",
    "    # gets input from the user \n",
    "    #=================================================================================================\n",
    "    longitude = float(input(\"Your Longitude, ex:-75.2: \"))\n",
    "    latitude = float(input(\"Your Latitude, ex:39.9: \"))\n",
    "    num_of_recommendations = int(input(\"How many number of recommendations: \"))\n",
    "    cluster = kmeans_city.predict(np.array([longitude,latitude]).reshape(1,-1))[0]\n",
    "    #=================================================================================================\n",
    "    # getting the number of number of restaurants for that city cluster \n",
    "    #=================================================================================================\n",
    "    restaurants = yelp_business_filtered[yelp_business_filtered['city_category']==cluster]\n",
    "    n_o_restaurants = restaurants.shape[0]\n",
    "    list_of_new_id = restaurants['business_id'].values.tolist()\n",
    "\n",
    "    if num_of_recommendations>n_o_restaurants:\n",
    "        #=============================================================================================\n",
    "        # You can tweak the radius a little to give the number of recommendations requested by user\n",
    "        #=============================================================================================\n",
    "        list_of_new_id = increase_radius(int(latitude),int(longitude),num_of_recommendations)\n",
    "        \n",
    "    y = yelp_business_filtered[yelp_business_filtered['business_id'].isin(list_of_new_id)] #.values.tolist()\n",
    "    y = y[:num_of_recommendations]\n",
    "    business_name_ = y['business_id'].tolist()\n",
    "    business_ = []\n",
    "    for i in range(len(business_name_)):\n",
    "        business_name = yelp_business_filtered['name'].loc[yelp_business_filtered['business_id'] == business_name_[i]]\n",
    "        business_.append(business_name.tolist()[0])\n",
    "    return business_\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9297f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = yelp_business_filtered[['latitude','longitude']]\n",
    "distortion_counter(2,30,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e0c1b6",
   "metadata": {},
   "source": [
    "## 2) Non Personalised Recommender System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dff9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sample.rename(columns={'business_id': \"businessId\", 'stars': \"rating\"})\n",
    "df = df[[\"businessId\",\"rating\"]]\n",
    "ratings_mean = df.groupby(['businessId'])[['rating']].mean().rename(columns = {'rating':'mean_rating'}).reset_index()\n",
    "ratings_sum = df.groupby(['businessId'])[['rating']].sum().rename(columns = {'rating':'sum_rating'}).reset_index()\n",
    "alpha = 1\n",
    "ratings_sum['sum_rating_factor'] = ratings_sum['sum_rating'] + alpha*(df['rating'].mean())\n",
    "ratings_count = df.groupby(['businessId'])[['rating']].count().rename(columns = {'rating':'count_rating'}).reset_index()\n",
    "ratings_count['count_rating_factor'] = ratings_count['count_rating'] + alpha\n",
    "ratings_damped = pd.merge(ratings_sum,ratings_count[['businessId','count_rating','count_rating_factor']], on=['businessId'], how='left')\n",
    "ratings_damped['damped_mean'] = ratings_damped['sum_rating_factor']/ratings_damped['count_rating_factor']\n",
    "rating_mean_dampmean = pd.merge(ratings_mean[['businessId','mean_rating']],ratings_damped[['businessId','damped_mean']], on =['businessId'], how= 'left')\n",
    "rating_mean_dampmean = rating_mean_dampmean.sort_values(['mean_rating'], ascending=False)\n",
    "\n",
    "#====================================================================\n",
    "# Call this function to make recommendation \n",
    "#====================================================================\n",
    "\n",
    "def non_personalised(n):\n",
    "    list_of_predictions = rating_mean_dampmean['businessId'][:n].tolist()\n",
    "    business_ = []\n",
    "    for i in range(len(list_of_predictions)):\n",
    "            business_name_ = list_of_predictions[i]\n",
    "            business_name = yelp_business_filtered['name'].loc[yelp_business_filtered['business_id'] == business_name_]\n",
    "            business_.append(business_name.tolist()[0])\n",
    "    for i in range(len(business_)):\n",
    "        pass\n",
    "        #print(\"Your number\",i+1,\"recommendation is\",business_[i])\n",
    "        \n",
    "    return business_\n",
    "\n",
    "#====================================================================\n",
    "# Model's loss function (MSE loss)\n",
    "#====================================================================\n",
    "\n",
    "\n",
    "mse = mean_squared_error(rating_mean_dampmean['mean_rating'], rating_mean_dampmean['damped_mean'])\n",
    "print(\"MSE of Non Personalised Recommender System\",mse)\n",
    "\n",
    "#====================================================================\n",
    "# Coverage\n",
    "#====================================================================\n",
    "\n",
    "list_of_cities = yelp_business_filtered['city'].unique()\n",
    "    \n",
    "\n",
    "# picking 5 users for each city with 100 businesses\n",
    "num_of_users_to_pick = []\n",
    "\n",
    "# values to be calculated for coverage\n",
    "val = []\n",
    "\n",
    "for i in range(len(list_of_cities)):\n",
    "    \n",
    "    # getting the number of business in a city\n",
    "    num_business = len(yelp_business_filtered[yelp_business_filtered['city']== list_of_cities[i]])\n",
    "    \n",
    "    print(\"num business:\", num_business)\n",
    "    if num_business >= 90:\n",
    "\n",
    "        test_user = int(10*(num_business/100))\n",
    "        num_of_users_to_pick.append(test_user)\n",
    "        #print(\"City\",list_of_cities[i], \"has\", num_business, \"businesses and their target test user\",test_user )\n",
    "        \n",
    "        # to get business_ids and user_id of each city\n",
    "        business_id = yelp_business_filtered['business_id'][yelp_business_filtered['city']== list_of_cities[i]]\n",
    "        business_id = business_id.tolist()\n",
    "        \n",
    "        \n",
    "        # to get business_ids and user_id of each city\n",
    "        user_id_all = (sample['user_id'][sample['business_id'].isin(business_id)]).tolist()\n",
    "        random.shuffle(user_id_all)\n",
    "        user_to_be_predictd = (user_id_all[:test_user])\n",
    "       \n",
    "        list_of_restaurant_prediction = []\n",
    "        \n",
    "        for i in range(len(user_to_be_predictd)):\n",
    "            prediction_10 = non_personalised(10)\n",
    "            list_of_restaurant_prediction.extend(prediction_10)\n",
    "            \n",
    "        num_of_recommended = len(np.unique(list_of_restaurant_prediction))\n",
    "        \n",
    "        num = (num_of_recommended/num_business)*100\n",
    "    \n",
    "        val.append(num)\n",
    "        \n",
    "from statistics import mean\n",
    "\n",
    "coverage = mean(val)\n",
    "\n",
    "print(coverage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de4914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serendipity():\n",
    "    users = sample['user_id'].tolist()\n",
    "    \n",
    "    for i in range(len(users)):\n",
    "        restaurant = []\n",
    "        business_id = sample['business_id'][sample['user_id'] == users[i]].tolist()\n",
    "        print(business_id)\n",
    "        for i in range(len(business_id)):\n",
    "            restaurant.append(yelp_business_filtered['name'][yelp_business_filtered['business_id']== business_id[i]].tolist())\n",
    "\n",
    "        if restaurant[0] in non_personalised(10):\n",
    "            print(\"True\")\n",
    "        \n",
    "        print(\"restaurant\", restaurant)\n",
    "        print('non_personalised(10)',non_personalised(10))\n",
    "        \n",
    "serendipity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009ce89",
   "metadata": {},
   "source": [
    "## 3) Personalised Recommender System - SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61dad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "#=======================================================================================\n",
    "# split the train test by date\n",
    "\n",
    "user_item_rating = sample[[\"user_id\", 'business_id', 'date', 'stars']]\n",
    "\n",
    "user_item_rating[\"date\"] = pd.to_datetime(user_item_rating[\"date\"])\n",
    "\n",
    "user_item_rating[\"date\"] = user_item_rating[\"date\"].astype('datetime64[ns]')\n",
    "\n",
    "user_item_rating.sort_values(by='date', inplace=False)\n",
    "\n",
    "#=======================================================================================\n",
    "\n",
    "train, test_val = train_test_split(user_item_rating, test_size=0.4,random_state=None, shuffle=False, stratify=None)\n",
    "\n",
    "val, test = train_test_split(test_val, test_size=0.5,random_state=None, shuffle=False, stratify=None)\n",
    "\n",
    "#=======================================================================================\n",
    "\n",
    "reader = Reader(rating_scale = (0.0, 5.0))\n",
    "\n",
    "trainset = train.loc[:,['user_id', 'business_id', 'stars']]\n",
    "\n",
    "trainset.columns = ['userID', 'itemID','rating']\n",
    "\n",
    "valset = val.loc[:,['user_id', 'business_id', 'stars']]\n",
    "\n",
    "valset.columns = ['userID', 'itemID','rating']\n",
    "\n",
    "testset = test.loc[:,['user_id', 'business_id', 'stars']]\n",
    "\n",
    "testset.columns = ['userID', 'itemID','rating']\n",
    "\n",
    "user_item_rating = user_item_rating.loc[:,['user_id', 'business_id', 'stars']]\n",
    "\n",
    "user_item_rating.columns = ['userID', 'itemID','rating']\n",
    "\n",
    "#=======================================================================================\n",
    "\n",
    "train_data = Dataset.load_from_df(trainset[['userID', 'itemID','rating']], reader)\n",
    "\n",
    "val_data = Dataset.load_from_df(valset[['userID', 'itemID','rating']], reader)\n",
    "\n",
    "test_data = Dataset.load_from_df(testset[['userID', 'itemID','rating']], reader)\n",
    "\n",
    "user_item_rating = Dataset.load_from_df(user_item_rating[['userID', 'itemID','rating']], reader)\n",
    "\n",
    "#=======================================================================================\n",
    "\n",
    "train_sr = train_data.build_full_trainset()\n",
    "\n",
    "val_sr_before = val_data.build_full_trainset()\n",
    "\n",
    "val_sr = val_sr_before.build_testset()\n",
    "\n",
    "test_sr_before = test_data.build_full_trainset()\n",
    "\n",
    "test_sr = test_sr_before.build_testset()\n",
    "\n",
    "user_item_rating = user_item_rating.build_full_trainset()\n",
    "\n",
    "#=====================================================================================\n",
    "# Hyperparameter Tuning with SVD model\n",
    "#=====================================================================================\n",
    "\n",
    "def tuning(train_sr,val_sr):\n",
    "    RMSE_tune = {}\n",
    "    n_epochs = [ 30, 40, 50, 60]  # the number of iteration of the SGD procedure\n",
    "    lr_all = [0.001, 0.003, 0.005, 0.01] # the learning rate for all parameters\n",
    "    reg_all =  [0.05, 0.1, 0.4, 0.5] # the regularization term for all parameters\n",
    "\n",
    "    for n in n_epochs:\n",
    "        for l in lr_all:\n",
    "            for r in reg_all:\n",
    "                #print('Fitting n: {0}, l: {1}, r: {2}'.format(n, l, r))\n",
    "                # SVD model initialized\n",
    "                algo = SVD(n_epochs = n, lr_all = l, reg_all = r)\n",
    "                # Training set is fit to the model\n",
    "                algo.fit(train_sr)\n",
    "                # Testing the algorithm\n",
    "                predictions = algo.test(val_sr)\n",
    "                # Geting the RMSE for the test set\n",
    "                RMSE_tune[n,l,r] = accuracy.rmse(predictions)\n",
    "    # Returns the min value of the dictionary\n",
    "    return min(RMSE_tune.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "#================================================================\n",
    "# Getting the best parameter\n",
    "#================================================================\n",
    "\n",
    "parameters = tuning(train_sr,val_sr)\n",
    "print(parameters)\n",
    "\n",
    "best_number_of_epochs = parameters[0]\n",
    "\n",
    "best_learning_rate = parameters[1]\n",
    "\n",
    "best_reg_all = parameters[2]\n",
    "\n",
    "print(best_number_of_epochs,best_learning_rate,best_reg_all)\n",
    "\n",
    "#===================================================================================================\n",
    "# training SVD algorithm in the optimal parameter \n",
    "#====================================================================================================\n",
    "\n",
    "algo_real = SVD(n_epochs = best_number_of_epochs, lr_all = best_learning_rate, reg_all = best_reg_all)\n",
    "\n",
    "algo_real.fit(train_sr)\n",
    "\n",
    "predictions = algo_real.test(test_sr)\n",
    "\n",
    "#================================================================\n",
    "# Evalution\n",
    "#================================================================\n",
    "\n",
    "print(\"RMSE for SVD\",accuracy.rmse(predictions))\n",
    "\n",
    "print(\"MAE for SVD\", accuracy.mae(predictions))\n",
    "\n",
    "print(\"R2 Score for SVD\",r2_score([t[2] for t in predictions], [t[3] for t in predictions]))\n",
    "\n",
    "#================================================================\n",
    "# SVD Recommendation\n",
    "#================================================================\n",
    "\n",
    "def SVD_recommend(user,num_of_recommendations):\n",
    "    list_of_predictions = []\n",
    "    unique_business = sample.business_id.unique()\n",
    "    business_ = []\n",
    "\n",
    "    for i in range(sample.business_id.nunique()):\n",
    "            business_id = unique_business[i]\n",
    "            y_hat = algo_real.predict(user, business_id).est\n",
    "            list_of_predictions.append([business_id,y_hat])\n",
    "\n",
    "    list_of_predictions.sort(key=lambda x: x[1],reverse = True)\n",
    "    list_of_predictions = list_of_predictions[:num_of_recommendations]\n",
    "    for i in range(len(list_of_predictions)):\n",
    "            business_name_ = list_of_predictions[i]\n",
    "            business_name = yelp_business_filtered['name'].loc[yelp_business_filtered['business_id'] == business_name_[0]]\n",
    "            business_.append(business_name.tolist()[0])\n",
    "\n",
    "    for i in range(len(business_)):\n",
    "            print(\"Your number\",i+1,\"recommendation is\",business_[i])\n",
    "\n",
    "#====================================================================\n",
    "# Calculating the coverage\n",
    "#====================================================================\n",
    "list_of_cities = yelp_business_filtered['city'].unique()\n",
    "\n",
    "\n",
    "# picking 5 users for each city with 100 businesses\n",
    "num_of_users_to_pick = []\n",
    "\n",
    "# values to be calculated for coverage\n",
    "val = []\n",
    "\n",
    "for i in range(len(list_of_cities)):\n",
    "    \n",
    "    # getting the number of business in a city\n",
    "    num_business = len(yelp_business_filtered[yelp_business_filtered['city']== list_of_cities[i]])\n",
    "    \n",
    "    if num_business >= 90:\n",
    "\n",
    "        test_user = int(10*(num_business/100))\n",
    "        num_of_users_to_pick.append(test_user)\n",
    "    \n",
    "        \n",
    "        # to get business_ids and user_id of each city\n",
    "        business_id = yelp_business_filtered['business_id'][yelp_business_filtered['city']== list_of_cities[i]]\n",
    "        business_id = business_id.tolist()\n",
    "        \n",
    "        \n",
    "        # to get business_ids and user_id of each city\n",
    "        user_id_all = (sample['user_id'][sample['business_id'].isin(business_id)]).tolist()\n",
    "        random.shuffle(user_id_all)\n",
    "        user_to_be_predictd = (user_id_all[:test_user])\n",
    "       \n",
    "        list_of_restaurant_prediction = []\n",
    "        \n",
    "        for i in range(len(user_to_be_predictd)):\n",
    "            prediction_10 = SVD_recommend(user_to_be_predictd[i],10)\n",
    "            print(prediction_10)\n",
    "            list_of_restaurant_prediction.extend(prediction_10)\n",
    "            \n",
    "        num_of_recommended = len(np.unique(list_of_restaurant_prediction))\n",
    "        \n",
    "        num = (num_of_recommended/num_business)*100\n",
    "    \n",
    "        val.append(num)\n",
    "        \n",
    "from statistics import mean\n",
    "\n",
    "coverage = mean(val)\n",
    "\n",
    "print(coverage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25482a11",
   "metadata": {},
   "source": [
    "## 4) Personalised Recommender System - Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6557fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection,metrics, preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#====================================================================\n",
    "# Getting the dataset ready\n",
    "#====================================================================\n",
    "\n",
    "df = sample[['user_id','business_id','stars']]\n",
    "\n",
    "df = df.rename(columns={'user_id':'userId','business_id':'businessId','stars':'rating'})\n",
    "\n",
    "df = df[:28214]\n",
    "\n",
    "#====================================================================\n",
    "# Training Dataset Class Wrapper \n",
    "#====================================================================\n",
    "\n",
    "class YelpDataset:\n",
    "    def __init__(self, users, business, ratings):\n",
    "        self.users = users\n",
    "        self.business = business\n",
    "        self.ratings = ratings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        users = self.users[item]\n",
    "        business = self.business[item]\n",
    "        ratings = self.ratings[item]\n",
    "        \n",
    "        return {\n",
    "            \"users\": torch.tensor(users, dtype=torch.long),\n",
    "            \"business\": torch.tensor(business, dtype=torch.long),\n",
    "            \"ratings\": torch.tensor(ratings, dtype=torch.long),\n",
    "        }\n",
    "    \n",
    "#====================================================================\n",
    "# Model Definiting \n",
    "#====================================================================\n",
    "\n",
    "class RecSysModel(nn.Module):\n",
    "    def __init__(self, n_users, n_business):\n",
    "        super().__init__()\n",
    "        # trainable lookup matrix for shallow embedding vectors\n",
    "    \n",
    "        self.user_embed = nn.Embedding(n_users, 32)\n",
    "        self.business_embed = nn.Embedding(n_business,32)\n",
    "        self.out = nn.Linear(64,1)\n",
    "        \n",
    "    def forward(self, users, business, rating=None):\n",
    "        user_embeds = self.user_embed(users)\n",
    "        business_embeds = self.business_embed(business)\n",
    "        output = torch.cat([user_embeds,business_embeds], dim=1)\n",
    "        \n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "#====================================================================\n",
    "# label encoder is a bit like getting all the uniques values\n",
    "# to make sure each userid fits into the total unique user Id\n",
    "#====================================================================\n",
    "\n",
    "lbl_user = preprocessing.LabelEncoder()\n",
    "lbl_business = preprocessing.LabelEncoder()\n",
    "\n",
    "#====================================================================\n",
    "# fit_transform() is used on the training data so that we can scale \n",
    "# the training data and also learn the scaling parameters of that data.\n",
    "#====================================================================\n",
    "\n",
    "df.userId = lbl_user.fit_transform(df.userId.values)\n",
    "df.businessId = lbl_business.fit_transform(df.businessId.values)\n",
    "\n",
    "#====================================================================\n",
    "# Getting the training and validating sets \n",
    "#====================================================================\n",
    "\n",
    "df_train, df_valid = model_selection.train_test_split(\n",
    "    df, test_size=0.1, random_state=None, stratify=df.rating.values)\n",
    "\n",
    "train_dataset = YelpDataset(\n",
    "    users = df_train.userId.values,\n",
    "    business = df_train.businessId.values,\n",
    "    ratings = df_train.rating.values)\n",
    "\n",
    "valid_dataset = YelpDataset(\n",
    "    users = df_valid.userId.values,\n",
    "    business = df_valid.businessId.values,\n",
    "    ratings = df_valid.rating.values)\n",
    "\n",
    "#====================================================================\n",
    "# Batch Processing in batches of 4\n",
    "#====================================================================\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                          batch_size = 4,\n",
    "                          shuffle = True,\n",
    "                          num_workers = 0)\n",
    "\n",
    "validation_loader = DataLoader(dataset = valid_dataset,\n",
    "                          batch_size = 4,\n",
    "                          shuffle = True,\n",
    "                          num_workers = 0)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "dataloader_data = dataiter.next()\n",
    "\n",
    "#====================================================================\n",
    "# Processing in GPU\n",
    "#====================================================================\n",
    "\n",
    "device = torch.device('cude' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = RecSysModel(\n",
    "    n_users = len(lbl_user.classes_),\n",
    "    n_business = len(lbl_business.classes_),\n",
    ").to(device)\n",
    "\n",
    "#====================================================================\n",
    "# Model's optimizer\n",
    "#====================================================================\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "sch = torch.optim.lr_scheduler.StepLR(optimizer, step_size =3, gamma=0.7)\n",
    "\n",
    "#====================================================================\n",
    "# Model's loss function (MSE loss)\n",
    "#====================================================================\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "user_embed = nn.Embedding(len(lbl_user.classes_),32)\n",
    "business_embed = nn.Embedding(len(lbl_business.classes_),32)\n",
    "\n",
    "out = nn.Linear(64,1)\n",
    "\n",
    "user_embeds = user_embed(dataloader_data['users'])\n",
    "business_embeds = business_embed(dataloader_data['business'])\n",
    "\n",
    "output = torch.cat([user_embeds,business_embeds], dim=1)\n",
    "output = out(output)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(dataloader_data['users'],\n",
    "                        dataloader_data['business'])\n",
    "\n",
    "rating = dataloader_data['ratings']\n",
    "\n",
    "#====================================================================\n",
    "# Model Training \n",
    "#====================================================================\n",
    "epochs = 1\n",
    "total_loss = 0\n",
    "plot_steps, print_steps = 100,100\n",
    "step_cnt = 0\n",
    "all_losses_list = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    \n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        \n",
    "        output = model(train_data[\"users\"],\n",
    "                      train_data[\"business\"])\n",
    "        rating = train_data[\"ratings\"].view(4,-1).to(torch.float32)\n",
    "        \n",
    "        loss = loss_func(output, rating)\n",
    "        total_loss = total_loss + loss.sum().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        step_cnt = step_cnt + len(train_data[\"users\"])\n",
    "        \n",
    "        if(step_cnt % plot_steps == 0):\n",
    "            avg_loss = total_loss/(len(train_data[\"users\"])*plot_steps)\n",
    "            all_losses_list.append(avg_loss)\n",
    "            total_loss = 0\n",
    "\n",
    "#====================================================================\n",
    "# Saving the weight into a pth file\n",
    "#====================================================================\n",
    "\n",
    "PATH = \"model_weights.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "#====================================================================\n",
    "# Loading the weight into the model\n",
    "#====================================================================\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#====================================================================\n",
    "# Evaluating the model to make predictions\n",
    "#====================================================================\n",
    "\n",
    "model.eval()\n",
    "\n",
    "#====================================================================\n",
    "# Call this function to make recommendation to a specific user\n",
    "#====================================================================\n",
    "\n",
    "def predict_best_restaurants(user,num_of_rec):\n",
    "    print(\"There are\",df.userId.nunique(),\"number of users and their ids range between 0 and\",df.userId.nunique()-1) \n",
    "    print('Recommendation for user_id:')\n",
    "    user = user #int(input())\n",
    "    print('How many recommendations would you like:')\n",
    "    num_of_rec = num_of_rec #int(input())\n",
    "    user = torch.tensor([user], dtype=torch.long)\n",
    "\n",
    "    list_of_predictions = []\n",
    "    business_ = []\n",
    "    unique_business = df.businessId.unique().tolist()\n",
    "    for i in range(df.businessId.nunique()):\n",
    "        business_id = unique_business[i]\n",
    "        business = torch.tensor([business_id], dtype=torch.long)\n",
    "        y_hat = model(user,business)\n",
    "        y_hat = y_hat.detach().numpy()\n",
    "        list_of_predictions.append([business_id,y_hat[0]])\n",
    "        \n",
    "    list_of_predictions.sort(key=lambda x: x[1],reverse = True)\n",
    "    list_of_predictions = list_of_predictions[:num_of_rec]\n",
    "\n",
    "    for i in range(len(list_of_predictions)):\n",
    "        business_name_ = list_of_predictions[i][0]\n",
    "        business_name = yelp_business_filtered['name'].loc[yelp_business_filtered['business_id'] == business_name_]\n",
    "        business_.append(business_name.tolist()[0])\n",
    "        \n",
    "    for i in range(len(business_)):\n",
    "        print(\"Your number\",i+1,\"recommendation is\",business_[i])\n",
    "    \n",
    "    return business_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d624e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================\n",
    "# Model's loss function (MSE loss) in a graph\n",
    "#====================================================================\n",
    "print(\"The minimum MSE score for the model is:\",min(all_losses_list) )\n",
    "print(min(all_losses_list))\n",
    "plt.figure()\n",
    "plt.plot(all_losses_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1515a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================\n",
    "# Calculating the coverage\n",
    "#====================================================================\n",
    "list_of_cities = yelp_business_filtered['city'].unique()\n",
    "\n",
    "\n",
    "# picking 5 users for each city with 100 businesses\n",
    "num_of_users_to_pick = []\n",
    "\n",
    "# values to be calculated for coverage\n",
    "val = []\n",
    "\n",
    "for i in range(len(list_of_cities)):\n",
    "    \n",
    "    # getting the number of business in a city\n",
    "    num_business = len(yelp_business_filtered[yelp_business_filtered['city']== list_of_cities[i]])\n",
    "    \n",
    "    print(\"num business:\", num_business)\n",
    "    if num_business >= 90:\n",
    "\n",
    "        test_user = int(10*(num_business/100))\n",
    "        num_of_users_to_pick.append(test_user)\n",
    "        print(\"City\",list_of_cities[i], \"has\", num_business, \"businesses and their target test user\",test_user )\n",
    "        \n",
    "        # to get business_ids and user_id of each city\n",
    "        business_id = yelp_business_filtered['business_id'][yelp_business_filtered['city']== list_of_cities[i]]\n",
    "        business_id = business_id.tolist()\n",
    "        \n",
    "        \n",
    "        # to get business_ids and user_id of each city\n",
    "        user_id_all = (sample['user_id'][sample['business_id'].isin(business_id)]).tolist()\n",
    "        random.shuffle(user_id_all)\n",
    "        user_to_be_predictd = (user_id_all[:test_user])\n",
    "       \n",
    "        list_of_restaurant_prediction = []\n",
    "        \n",
    "        for i in range(len(user_to_be_predictd)):\n",
    "            prediction_10 = predict_best_restaurants(user_to_be_predictd[i],10)\n",
    "            list_of_restaurant_prediction.extend(prediction_10)\n",
    "            \n",
    "        num_of_recommended = len(np.unique(list_of_restaurant_prediction))\n",
    "        \n",
    "        num = (num_of_recommended/num_business)*100\n",
    "    \n",
    "        val.append(num)\n",
    "        \n",
    "from statistics import mean\n",
    "\n",
    "coverage = mean(val)\n",
    "\n",
    "print(coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52f3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d236f415f651a83033f3a49992b7b8a4f3fe66679b73e15de5ecbd445b5e0975"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
